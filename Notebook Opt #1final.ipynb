{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('unsup_predict': conda)"
    },
    "interpreter": {
      "hash": "50b37eb35d55879b46f571aef5e85bb73790ee1370826879b0c77563c93e6dfb"
    },
    "colab": {
      "name": "Notebook Opt #1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qUra-j1I5b_"
      },
      "source": [
        "# EDSA Unsupervised Learning Sprint\n",
        "\n",
        "### The team:\n",
        "- Thapelo Makhalanyane\n",
        "- Kabelo Mbewe\n",
        "- Reitumetse Nchoe\n",
        "- Mpho Nonyane\n",
        "- Hercules Smith\n",
        "\n",
        "### The Goal:\n",
        "- To build a model than can produce an estimated rating for given user and movie combination (that doesn't yet exist) quickly and accurately. This part of the project focuses on the best possible model performance, within reason (we cannot train until the heat-death of the universe, for example)\n",
        "\n",
        "## Steps:\n",
        "1. Bring in our data and Explore it a bit, gathering some insights about our data.\n",
        "2. Format our data in a way that allows us to perform unsupervised learning tasks.\n",
        "3. Build and evaluate some models\n",
        "4. Refine our favourite model\n",
        "5. Produce a submission\n",
        "6. Conclusion\n",
        "\n",
        "## Data Overview\n",
        "This dataset consists of several million 5-star ratings obtained from users of the online MovieLens movie recommendation service. The MovieLens dataset has long been used by industry and academic researchers to improve the performance of explicitly-based recommender systems, and now you get to as well!\n",
        "\n",
        "For this Predict, we'll be using a special version of the MovieLens dataset which has enriched with additional data, and resampled for fair evaluation purposes.\n",
        "\n",
        "## Source\n",
        "The data for the MovieLens dataset is maintained by the GroupLens research group in the Department of Computer Science and Engineering at the University of Minnesota. Additional movie content data was legally scraped from IMDB\n",
        "\n",
        "## Supplied Files\n",
        "- `genome_scores.csv` - a score mapping the strength between movies and tag-related properties.\n",
        "- `genome_tags.csv` - user assigned tags for genome-related scores\n",
        "- `imdb_data.csv` - Additional movie metadata scraped from IMDB using the `links.csv` file.\n",
        "- `links.csv` - File providing a mapping between a MovieLens ID and associated IMDB and TMDB IDs.\n",
        "- `tags.csv` - User assigned for the movies within the dataset.\n",
        "- `test.csv` - The test split of the dataset. Contains user and movie IDs with no rating data.\n",
        "- `train.csv` - The training split of the dataset. Contains user and movie IDs with associated rating data.\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "# Additional Information\n",
        "The below information is provided directly from the MovieLens dataset description files:\n",
        "\n",
        "### Ratings Data File Structure (`train.csv`)\n",
        "All ratings are contained in the file train.csv. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:\n",
        "\n",
        "```\n",
        "userId,movieId,rating,timestamp\n",
        "```\n",
        "\n",
        "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
        "\n",
        "Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
        "\n",
        "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
        "\n",
        "### Tags Data File Structure (`tags.csv`)\n",
        "All tags are contained in the file tags.csv. Each line of this file after the header row represents one tag applied to one movie by one user, and has the following format:\n",
        "\n",
        "```\n",
        "userId,movieId,tag,timestamp\n",
        "```\n",
        "\n",
        "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
        "\n",
        "Tags are user-generated metadata about movies. Each tag is typically a single word or short phrase. The meaning, value, and purpose of a particular tag is determined by each user.\n",
        "\n",
        "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970\n",
        "\n",
        "### Movies Data File Structure (`movies.csv`)\n",
        "Movie information is contained in the file movies.csv. Each line of this file after the header row represents one movie, and has the following format:\n",
        "\n",
        "```\n",
        "movieId,title,genres\n",
        "```\n",
        "\n",
        "Movie titles are entered manually or imported from https://www.themoviedb.org/, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.\n",
        "\n",
        "Genres are a pipe-separated list, and are selected from the following:\n",
        "\n",
        "- Action\n",
        "- Adventure\n",
        "- Animation\n",
        "- Children's\n",
        "- Comedy\n",
        "- Crime\n",
        "- Documentary\n",
        "- Drama\n",
        "- Fantasy\n",
        "- Film-Noir\n",
        "- Horror\n",
        "- Musical\n",
        "- Mystery\n",
        "- Romance\n",
        "- Sci-Fi\n",
        "- Thriller\n",
        "- War\n",
        "- Western\n",
        "- (no genres listed)\n",
        "\n",
        "### Links Data File Structure (`links.csv`)\n",
        "Identifiers that can be used to link to other sources of movie data are contained in the file links.csv. Each line of this file after the header row represents one movie, and has the following format:\n",
        "\n",
        "```\n",
        "movieId,imdbId,tmdbId\n",
        "```\n",
        "\n",
        "movieId is an identifier for movies used by https://movielens.org. E.g., the movie Toy Story has the link https://movielens.org/movies/1.\n",
        "\n",
        "imdbId is an identifier for movies used by http://www.imdb.com. E.g., the movie Toy Story has the link http://www.imdb.com/title/tt0114709/.\n",
        "\n",
        "tmdbId is an identifier for movies used by https://www.themoviedb.org. E.g., the movie Toy Story has the link https://www.themoviedb.org/movie/862.\n",
        "\n",
        "Use of the resources listed above is subject to the terms of each provider.\n",
        "\n",
        "### Tag Genome (genome-scores.csv and `genome_tags.csv`)\n",
        "As described in this article, the tag genome encodes how strongly movies exhibit particular properties represented by tags (atmospheric, thought-provoking, realistic, etc.). The tag genome was computed using a machine learning algorithm on user-contributed content including tags, ratings, and textual reviews.\n",
        "\n",
        "The genome is split into two files. The file `genome_scores.csv` contains movie-tag relevance data in the following format:\n",
        "\n",
        "```\n",
        "movieId,tagId,relevance\n",
        "```\n",
        "The second file, `genome_tags.csv`, provides the tag descriptions for the tag IDs in the genome file, in the following format:\n",
        "\n",
        "```\n",
        "tagId,tag\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7GjX7yyI5cP"
      },
      "source": [
        "# Step 1: Show me the data\n",
        "\n",
        "(we start with imports)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn4_x2-iI5cU"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from plotly import express as px"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8Q31prwI5cY"
      },
      "source": [
        "## helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6id4cGDFI5cc"
      },
      "source": [
        "def demo_frame(df: pd.DataFrame):\n",
        "    print(df.info())\n",
        "    return df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h_zuRR3I5ce"
      },
      "source": [
        "base_path = \"./data\"\n",
        "genome_scores = pd.read_csv(f'{base_path}/genome_scores.csv')\n",
        "genome_tags = pd.read_csv(f'{base_path}/genome_tags.csv')\n",
        "imdb_data = pd.read_csv(f'{base_path}/imdb_data.csv')\n",
        "# links = pd.read_csv(f'{base_path}/links.csv')  # we won't be using links at all\n",
        "movies = pd.read_csv(f'{base_path}/movies.csv')\n",
        "tags = pd.read_csv(f'{base_path}/tags.csv')\n",
        "test = pd.read_csv(f'{base_path}/test.csv')\n",
        "train = pd.read_csv(f'{base_path}/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQTBbwpqI5ch"
      },
      "source": [
        "We'll start our journey by exploring the `genome` data, since that sounds the most interesting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELZcm6C9I5ck"
      },
      "source": [
        "demo_frame(genome_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnCBHy__I5cn"
      },
      "source": [
        "demo_frame(genome_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ptim-Z6I5cp"
      },
      "source": [
        "Every movie is tagged with one of 1'128 tags, each movie-tag pair has a `relevance` score which seems to indicate how *strongly* the `movie` is associated with the `tag`.\n",
        "\n",
        "To use this data we may have to perform a `merge` and a `pivot`.\n",
        "All code tha combines / cleans data in any way is in the `scripts` folder under `data_cleaning.py`. The functions here generates new csv files and places them in a folder called `cleaned` so we don't have to waste processor cycles recleaning the same data over-and-over.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L-Ex8LNI5cr"
      },
      "source": [
        "genome_tag_vec = pd.read_csv('cleaned/genome_tag_vec.csv')\n",
        "\n",
        "demo_frame(genome_tag_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKqthiQVI5ct"
      },
      "source": [
        "We know `Toy Story (1995)` has `movieId == 1`, so let's see what top 10 tags are most relevant to it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC06Gj9UI5cu"
      },
      "source": [
        "sorted_tags = genome_tag_vec[genome_tag_vec.movieId == 1].T.sort_values(0, ascending=False)\n",
        "sorted_tags.iloc[1:11]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvKie5a5I5cw"
      },
      "source": [
        "All of these are very agreeable tags! We might end up using this data to improve model performance later. For now, let's keep Exploring! We'll look at the IMDB data next:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0-ZUdW1I5cx"
      },
      "source": [
        "demo_frame(imdb_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiqEzDpkI5cz"
      },
      "source": [
        "Pretty self-explanatory; there is a need to clean the dataframe a bit - splitting on the pipe (|) and transforming `budget` to be numeric. Let's see what that looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdOK8mnJI5cz"
      },
      "source": [
        "imdb_data = pd.read_csv('cleaned/imdb_data.csv')\n",
        "\n",
        "demo_frame(imdb_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_46pu_nMI5c0"
      },
      "source": [
        "Now we can mine some data about our movies!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EGUGY2kI5c1"
      },
      "source": [
        "from collections import Counter\n",
        "from scripts.helpers import flatten_list\n",
        "\n",
        "unique_actors_per_movie = imdb_data.title_cast.str.split('|').fillna(\"\").apply(set)\n",
        "print(\"Median # of actors per movie:\", unique_actors_per_movie.apply(len).median().round().astype(int))\n",
        "print(\"Total # of actors (in dataset):\", len(set.union(*unique_actors_per_movie)))\n",
        "\n",
        "actor_by_movies = pd.Series(Counter(flatten_list(unique_actors_per_movie))).sort_values(ascending=True)\n",
        "\n",
        "fig = px.bar(\n",
        "    data_frame=actor_by_movies.iloc[-20:], \n",
        "    x=0, \n",
        "    height=640,\n",
        "    labels={\"0\": \"# of Movies\"}, \n",
        "    orientation='h', \n",
        "    title='Number of movies per actor'\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVpW4mYxI5c3"
      },
      "source": [
        "director_by_movies = pd.Series(Counter(imdb_data.director)).sort_values(ascending=True)\n",
        "\n",
        "fig = px.bar(\n",
        "    data_frame=director_by_movies.iloc[-20:-2], \n",
        "    x=0, \n",
        "    height=640,\n",
        "    labels={\"0\": \"# of Movies\"}, \n",
        "    orientation='h', \n",
        "    title='Number of movies per director'\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rOUFOzAI5c4"
      },
      "source": [
        "movie_keywords = imdb_data.plot_keywords.str.split('|').fillna(\"\").apply(set)\n",
        "keyword_by_movies = pd.Series(Counter(flatten_list(movie_keywords))).sort_values(ascending=True)\n",
        "\n",
        "fig = px.bar(\n",
        "    data_frame=keyword_by_movies.iloc[-20:], \n",
        "    x=0, \n",
        "    height=640,\n",
        "    labels={\"0\": \"# of Movies\"}, \n",
        "    orientation='h', \n",
        "    title='Top Keywords'\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQg7eNOtI5c5"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import codecs\n",
        "import imageio\n",
        "import base64\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "mask = imageio.imread('assets/wc_mask.jpg')\n",
        "\n",
        "def gen_wordcloud(word_freq, title, savefig=False):\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    wc = WordCloud(colormap='magma', mask=mask, background_color=None, max_words=1_000, mode=\"RGBA\")\n",
        "    wc.generate_from_frequencies(word_freq)\n",
        "    plt.title(title, fontsize=12)\n",
        "    plt.axis(\"off\")\n",
        "    plt.margins(tight=True)\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    if savefig:\n",
        "        plt.savefig(f'{title}.png', transparent=True, bbox_inches='tight', dpi=200)\n",
        "    plt.show()\n",
        "    \n",
        "gen_wordcloud(keyword_by_movies, \"Most Common Words (Overall)\", savefig=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5ZSyfYjI5c7"
      },
      "source": [
        "It seems people are into some... people like what they like ;)\n",
        "\n",
        "Let's quickly take a look at `movies.csv`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T38sr1NI5c8"
      },
      "source": [
        "demo_frame(movies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTns89JYI5c9"
      },
      "source": [
        "It would be great to extract the year from the title... if only there was a pre-cleaned version to look at..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JtmFUCeI5c-"
      },
      "source": [
        "# hah, there is!\n",
        "movies= pd.read_csv('cleaned/movies.csv')\n",
        "\n",
        "demo_frame(movies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpO3grfZI5c-"
      },
      "source": [
        "Excellent! Genres aren't too interesting, let's take a quick peak at movie releases per year and move on!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs0rLo9II5c_"
      },
      "source": [
        "fig = px.histogram(movies, x='year', nbins=100, title='Movies released per year')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2bp5oEKI5dA"
      },
      "source": [
        "Not really any major insights here... The number of movies released per year is increasing over time - but that's relatively obvious. Later on, once we've brought in `train.csv` (which contains some timestamp info) we might be able to draw more insightful... insights.\n",
        "\n",
        "Let's keep Exploring - `tags.csv` is next. We'll give it the same treatment we gave the plot keywords above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQkM7hsXI5dB"
      },
      "source": [
        "gen_wordcloud(pd.Series(Counter(tags.tag)), \"Most Common Words (Overall)\", savefig=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_w7twvzI5dC"
      },
      "source": [
        "There are definitely some interesting tags here! We could potentially find similar users by how they tagged movies in some way! Let's carry on; The real meat of our data - the `train.csv` file!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNYefD6GI5dD"
      },
      "source": [
        "demo_frame(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "envwWq4-I5dD"
      },
      "source": [
        "Let's first how many movies each user has rated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV_GLRWTI5dE"
      },
      "source": [
        "rating_count = train.groupby('userId').movieId.count().sort_values(ascending=False)\n",
        "print(rating_count.iloc[:10])\n",
        "print(rating_count.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wENrMVjI5dF"
      },
      "source": [
        "Our top user has rated almost 13 thousand movies! the average number of ratings per user is 61, with the 25th percentile being 14 ratings. The median number of ratings is 28.\n",
        "\n",
        "Considering ratings as features of users, to help recommend new content (Collaborative Filtering) definitely seems like a good idea as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrtuEo7eI5dG"
      },
      "source": [
        "train.rating.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk8zR8FnI5dH"
      },
      "source": [
        "The median rating given is 3.5, the mean is only slightly more at 3.53.\n",
        "\n",
        "Now about those *\"more insightful insights\"*, let's see if there is any relationship between the age of a movie and the rating given:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A1-RXy9I5dH"
      },
      "source": [
        "train['timestamp'] = pd.to_datetime(train[\"timestamp\"], unit='s')\n",
        "demo_frame(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJBqqV_lI5dI"
      },
      "source": [
        "combined = train.merge(movies[['movieId', 'year']], on='movieId', how='left')\n",
        "combined['delay'] = train.timestamp.dt.year - combined.year\n",
        "\n",
        "combined.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwdIMp34I5dJ"
      },
      "source": [
        "Now we can easily, per movie, see how it's ratings change over time (if at all)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWtKdLpPI5dL"
      },
      "source": [
        "subset = combined[(combined.year > 1950)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms8klOtGI5dM"
      },
      "source": [
        "by_delay = subset.groupby(['movieId', 'delay'], as_index=False).rating.agg(['mean', 'count'])\n",
        "subset = by_delay[by_delay['count'] > 100].reset_index()\n",
        "subset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkXW4BWFI5dM"
      },
      "source": [
        "subset['rating_diff'] = subset.groupby('movieId')['mean'].diff(1).fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7YNichCI5dN"
      },
      "source": [
        "fig = px.scatter(\n",
        "    subset,\n",
        "    x='delay',\n",
        "    y='rating_diff',\n",
        "    color='rating_diff',\n",
        "    title='Change in rating over time'\n",
        ")\n",
        "fig.update_layout(showlegend=False)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnfnSs07I5dO"
      },
      "source": [
        "What this colorful mess above is saying is: Movie ratings don't tend to change very much over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejwaxMI0I5dP"
      },
      "source": [
        "# Step 2: Preprocessing\n",
        "\n",
        "We've done most of our preprocessing in our `data_cleaning.py` script. We include the code here for convenience:\n",
        "\n",
        "```python\n",
        "def clean_genome():\n",
        "    genome_scores = pd.read_csv(\"data/genome_scores.csv\")\n",
        "    genome_tags = pd.read_csv(\"data/genome_tags.csv\")\n",
        "\n",
        "    genome_data = genome_scores.merge(genome_tags, on=\"tagId\")\n",
        "\n",
        "    genome_tag_vec = genome_data.pivot(\n",
        "        index=\"movieId\", columns=\"tag\", values=\"relevance\"\n",
        "    )\n",
        "    genome_tag_vec[\"movieId\"] = genome_tag_vec.index\n",
        "\n",
        "    cols = [\"movieId\"] + [col for col in genome_tag_vec.columns if col != \"movieId\"]\n",
        "    genome_tag_vec = genome_tag_vec[cols]\n",
        "    genome_tag_vec.reset_index(inplace=True, drop=True)\n",
        "    genome_tag_vec.to_csv(\n",
        "        \"cleaned/genome_tag_vec.csv\",\n",
        "        index=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def clean_imdb():\n",
        "    imdb_data = pd.read_csv(\"data/imdb_data.csv\")\n",
        "    budget_to_num = lambda budget: int(\"\".join([c for c in budget if c.isnumeric()]))\n",
        "\n",
        "    imdb_data[\"budget\"] = imdb_data[\"budget\"].apply(\n",
        "        lambda b: np.nan if b is np.nan or b[0] != \"$\" else budget_to_num(b)\n",
        "    )\n",
        "    imdb_data.title_cast.fillna(\"\", inplace=True)\n",
        "    imdb_data.director.fillna(\"\", inplace=True)\n",
        "    imdb_data.runtime.fillna(int(imdb_data.runtime.median()), inplace=True)\n",
        "    imdb_data.budget.fillna(int(imdb_data.budget.median()), inplace=True)\n",
        "    imdb_data.plot_keywords.fillna(\"\", inplace=True)\n",
        "\n",
        "    imdb_data.to_csv(\n",
        "        \"cleaned/imdb_data.csv\",\n",
        "        index=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def clean_movies():\n",
        "    movies = pd.read_csv(\"data/movies.csv\")\n",
        "\n",
        "    movies[\"year\"] = (\n",
        "        movies.title.str.strip()\n",
        "        .str[-6:]\n",
        "        .str.extract(r\"\\(([0-9]{4})\\)\")\n",
        "        .fillna(0)\n",
        "        .astype(int)\n",
        "    )\n",
        "\n",
        "    normal_year_mean = movies[movies.year != 0].year.median()\n",
        "    movies[\"year\"].replace(0, normal_year_mean, inplace=True)\n",
        "\n",
        "    movies[\"title\"] = movies.title.str.strip().str.replace(\n",
        "        r\"\\(([0-9]{4})\\)\", \"\", regex=True\n",
        "    )\n",
        "    movies[\"title\"] = (\n",
        "        movies[\"title\"].str.strip().str.replace(r\"(.*), The$\", r\"The \\1\", regex=True)\n",
        "    )\n",
        "    movies[\"genres\"] = movies.genres.str.replace(\n",
        "        \"(no genres listed)\", \"unknown\", regex=False\n",
        "    )\n",
        "\n",
        "    movies.to_csv(\n",
        "        \"cleaned/movies.csv\",\n",
        "        index=False,\n",
        "    )\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePHEp-DwI5dR"
      },
      "source": [
        "# Step 3: Bring in the models!\n",
        "\n",
        "Finally! Am I right? First we need to talk about the types of models for recommendations:\n",
        "\n",
        "There are in essence 3:\n",
        "- Content-based filtering\n",
        "- Collaborative filtering\n",
        "- Hybrid (combine the 2 above + some other stuff)\n",
        "\n",
        "Scary-sounding stuff right? Basically; All recommendation is about showing a user things they'll like. How do we know what they'll like?\n",
        "- based on what else they've liked, ie. we find *content* that is similar to what they've already liked\n",
        "- based on what similar people have liked, ie. our users *collaborate* (work together) to suggest things to each other (indirectly)\n",
        "\n",
        "How we determine similarity is largely an embedding problem with some math sprinkled in - we don't need to get into too much details. In essence we try to represent each *thing* as a vector (think like a 2d (x, y) point in space, but with more dimentions), and then through the magic of math & information theory things that are similar *should* be close together (either measure by distance between them or angle between them).\n",
        "\n",
        "\n",
        "We'll start off by looking at **collaborative models**\n",
        "## Collaborative Filtering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9LZAn-QI5dS"
      },
      "source": [
        "# basic models\n",
        "from surprise import NormalPredictor, BaselineOnly\n",
        "# knn based models\n",
        "from surprise import KNNBasic, KNNWithMeans, KNNBaseline\n",
        "# matrix factorization models\n",
        "from surprise import SVD, SVDpp, NMF\n",
        "# other clustering models\n",
        "from surprise import SlopeOne, CoClustering\n",
        "\n",
        "# some tools to bring in our data\n",
        "from surprise import Dataset, Reader\n",
        "from surprise.model_selection import cross_validate, GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr5H43_7I5dT"
      },
      "source": [
        "We'll start off by using only the default hyperparameters for all models, and comparing them on the MovieLens 100k dataset. Then we'll compare them on a random sample of our data; finally we'll pick about 3 models to GridSearch, find the best parameters, and then cross validate them on the full training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF5R1W2GI5dT"
      },
      "source": [
        "models = (\n",
        "    NormalPredictor, BaselineOnly, \n",
        "    KNNBasic, KNNWithMeans, KNNBaseline, \n",
        "    SVD, SVDpp, NMF, \n",
        "    SlopeOne, CoClustering\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhd5yqMFI5dU"
      },
      "source": [
        "ml100k = Dataset.load_builtin('ml-100k')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4TIPDYzI5dV"
      },
      "source": [
        "results = {\n",
        "    \"model\": [],\n",
        "    \"train_rmse_mean\": [],\n",
        "    \"train_rmse_std\": [],\n",
        "    \"test_rmse_mean\": [],\n",
        "    \"test_rmse_std\": [],\n",
        "    \"fit_time_mean\": [],\n",
        "    \"fit_time_std\": [],\n",
        "    \"test_time_mean\": [],\n",
        "    \"test_time_std\": [],\n",
        "}\n",
        "for idx, model in enumerate(models):\n",
        "    print(idx, model.__name__)\n",
        "    result = cross_validate(\n",
        "        algo=model(), \n",
        "        data=ml100k, \n",
        "        measures=['rmse'], \n",
        "        n_jobs=-1, \n",
        "        return_train_measures=True\n",
        "    )\n",
        "    results['model'].append(model.__name__)\n",
        "    results['train_rmse_mean'].append(result['train_rmse'].mean())\n",
        "    results['train_rmse_std'].append(result['train_rmse'].std())\n",
        "\n",
        "    results['test_rmse_mean'].append(result['test_rmse'].mean())\n",
        "    results['test_rmse_std'].append(result['test_rmse'].std())\n",
        "\n",
        "    results['fit_time_mean'].append(np.array(result['fit_time']).mean())\n",
        "    results['fit_time_std'].append(np.array(result['fit_time']).std())\n",
        "\n",
        "    results['test_time_mean'].append(np.array(result['test_time']).mean())\n",
        "    results['test_time_std'].append(np.array(result['test_time']).std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET8fEDrrI5dW"
      },
      "source": [
        "result_df = pd.DataFrame(results).set_index('model', drop=True)\n",
        "result_df.sort_values(\"test_rmse_mean\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7AjrkTbI5dX"
      },
      "source": [
        "Clearly we don't need to consider the `NormalPredictor` (which is essentially just random guessing) any further. We might also exclude all models that performed worse than `BaselineOnly`, since it's just guessing means - doing worse than this isn't really valuable.\n",
        "\n",
        "We should also consider overfitting - let's look at the difference between train and test RMSE across all models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTuRPi2aI5dX"
      },
      "source": [
        "(result_df.train_rmse_mean - result_df.test_rmse_mean).abs().sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki4_Qg4bI5dY"
      },
      "source": [
        "Clearly All our complex - read matrix factorization - models are overfitting quite a bit! Thankfully this can be alleviated by increasing the regularisation hyperparamter; something we'll get to later.\n",
        "\n",
        "Let's first run the same test on the MovieLens 1M dataset - that should prove insightful too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNnNDRiZI5dZ"
      },
      "source": [
        "ml1m = Dataset.load_builtin('ml-1m')\n",
        "\n",
        "results_1m = {\n",
        "    \"model\": [],\n",
        "    \"train_rmse_mean\": [],\n",
        "    \"train_rmse_std\": [],\n",
        "    \"test_rmse_mean\": [],\n",
        "    \"test_rmse_std\": [],\n",
        "    \"fit_time_mean\": [],\n",
        "    \"fit_time_std\": [],\n",
        "    \"test_time_mean\": [],\n",
        "    \"test_time_std\": [],\n",
        "}\n",
        "for idx, model in enumerate(models):\n",
        "    print(idx, model.__name__)\n",
        "    result = cross_validate(\n",
        "        algo=model(), \n",
        "        data=ml1m, \n",
        "        measures=['rmse'], \n",
        "        n_jobs=-1, \n",
        "        return_train_measures=True\n",
        "    )\n",
        "    results_1m['model'].append(model.__name__)\n",
        "    results_1m['train_rmse_mean'].append(result['train_rmse'].mean())\n",
        "    results_1m['train_rmse_std'].append(result['train_rmse'].std())\n",
        "\n",
        "    results_1m['test_rmse_mean'].append(result['test_rmse'].mean())\n",
        "    results_1m['test_rmse_std'].append(result['test_rmse'].std())\n",
        "\n",
        "    results_1m['fit_time_mean'].append(np.array(result['fit_time']).mean())\n",
        "    results_1m['fit_time_std'].append(np.array(result['fit_time']).std())\n",
        "\n",
        "    results_1m['test_time_mean'].append(np.array(result['test_time']).mean())\n",
        "    results_1m['test_time_std'].append(np.array(result['test_time']).std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj21aP2rI5da"
      },
      "source": [
        "result_df_1m = pd.DataFrame(results_1m).set_index('model', drop=True)\n",
        "result_df_1m.sort_values(\"test_rmse_mean\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av04KAFJI5da"
      },
      "source": [
        "(result_df.train_rmse_mean - result_df.test_rmse_mean).abs().sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQT60c_xI5db"
      },
      "source": [
        "We see roughly the same amount of overfitting, with drastically better accuracy scores. One thing to note is that our amount of data is roughly 10x more, let's see how much longer the models took to train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWEA17CnI5dc"
      },
      "source": [
        "((result_df_1m - result_df) / result_df).sort_values('test_rmse_mean', ascending=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chWiqAALI5dd"
      },
      "source": [
        "Some models took **much** longer than others, specifically `KNNBasic` and `KNNWithMeans`. A quick look at the code reveals that both of these are implemented in pure python - so there is much room for improvement.\n",
        "\n",
        "Still though, our only reasonable options seem to be `SVD` and `SVDpp`. `SVDpp` trains **much** more slowly than `SVD`, but at least it shows similar algorithmic complexity while providing accurate results.\n",
        "\n",
        "Next let's explore content-based models:\n",
        "\n",
        "## Content-based Filtering\n",
        "\n",
        "We'll first combine all of the data we have on any movie into a single massive dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7hpRgh9I5dd"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from scipy import sparse\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwEvjmUaI5de"
      },
      "source": [
        "full_combine = (\n",
        "    movies\n",
        "    .merge(imdb_data,      on=\"movieId\", how=\"left\")\n",
        "    .merge(genome_tag_vec, on=\"movieId\", how=\"left\")\n",
        "    .set_index(\"movieId\", drop=True)\n",
        ")\n",
        "\n",
        "normal_year_mean = full_combine[full_combine.year != 0].year.mean()\n",
        "full_combine['year'].replace(0, normal_year_mean, inplace=True)\n",
        "\n",
        "# movies.year.fillna(int(movies.year.median()), inplace=True)\n",
        "# movies.genres.fillna(\"<unknown>\", inplace=True)\n",
        "full_combine.title = full_combine.title.str.strip().str.replace(r\"(.*), The$\", r\"The \\1\", regex=True)\n",
        "full_combine.title_cast.fillna(\"\", inplace=True)\n",
        "full_combine.director.fillna(\"\", inplace=True)\n",
        "full_combine.runtime.fillna(int(full_combine.runtime.median()), inplace=True)\n",
        "full_combine.budget.fillna(int(full_combine.budget.median()), inplace=True)\n",
        "full_combine.plot_keywords.fillna(\"\", inplace=True)\n",
        "\n",
        "full_combine['cast_size'] = full_combine.title_cast.str.split('|').apply(len)\n",
        "full_combine['genre_count'] = full_combine.genres.str.split('|').apply(len)\n",
        "\n",
        "movie_groups = train.set_index('movieId', drop=True).groupby('movieId')\n",
        "full_combine['rating_mean'] = movie_groups.rating.mean()\n",
        "full_combine['rating_std'] = movie_groups.rating.std().fillna(0)\n",
        "\n",
        "q3 = movie_groups.rating.quantile(0.75)\n",
        "q1 = movie_groups.rating.quantile(0.25)\n",
        "\n",
        "full_combine['rating_iqr'] =  q3 - q1\n",
        "full_combine['rating_count'] = movie_groups.apply(len)\n",
        "\n",
        "full_combine.fillna(0, inplace=True)\n",
        "\n",
        "full_combine['movieId'] = full_combine.index.values\n",
        "full_combine.reset_index(inplace=True, drop=True)\n",
        "full_combine.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UPJjc-lI5df"
      },
      "source": [
        "This humongus dataframe allows us to easily build some feature vectors; first we'll build one for genres:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axMyW2jHI5dg"
      },
      "source": [
        "features = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3NPYwfKI5dh"
      },
      "source": [
        "genre_vectrz = CountVectorizer(token_pattern=r\"[A-z\\-]+\", min_df=2)\n",
        "genre_vec = genre_vectrz.fit_transform(full_combine.genres)\n",
        "print(\"Genre Tokens:\", len(genre_vectrz.get_feature_names()))\n",
        "features.extend(genre_vectrz.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Q4k3_RI5dh"
      },
      "source": [
        "We'll repeat this process for cast, director, and plot keywords, and we'll even sprinkle in some of those genome tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zqfoGFAI5di"
      },
      "source": [
        "cast_vectrz = CountVectorizer(token_pattern=r\"[^\\|]+\", min_df=20)\n",
        "cast_vec = cast_vectrz.fit_transform(full_combine.title_cast)\n",
        "print(\"Cast Tokens:\", len(cast_vectrz.get_feature_names()))\n",
        "features.extend(cast_vectrz.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5h0uHYeI5dj"
      },
      "source": [
        "director_vectrz = CountVectorizer(token_pattern=r\".+\", min_df=10, stop_words=['see full summary'])\n",
        "director_vec = director_vectrz.fit_transform(full_combine.director)\n",
        "print(\"Director Tokens:\", len(director_vectrz.get_feature_names()))\n",
        "features.extend(director_vectrz.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBuawp6eI5dj"
      },
      "source": [
        "plot_vectrz = CountVectorizer(token_pattern=r\"[^\\|]+\", min_df=10, stop_words=stopwords)\n",
        "plot_vec = plot_vectrz.fit_transform(full_combine.plot_keywords)\n",
        "print(\"Plot KW Tokens:\", len(plot_vectrz.get_feature_names()))\n",
        "features.extend(plot_vectrz.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sIqA6bQI5dk"
      },
      "source": [
        "genome_features = [col for col in genome_tag_vec.columns if col != 'movieId']\n",
        "features.extend(genome_features)\n",
        "sparse_genome = sparse.csr_matrix(full_combine[genome_features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiiDI5ULI5dl"
      },
      "source": [
        "Now we can easily combined all of these different sparse matrices to produce on final one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brm8VMx5I5dl"
      },
      "source": [
        "extra_features = [\"year\", \"runtime\", \"budget\", \"rating_mean\", \"rating_std\", \"rating_iqr\", \"rating_count\"]\n",
        "features.extend(extra_features)\n",
        "\n",
        "extra_features = full_combine[extra_features]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "transformed = scaler.fit_transform(extra_features)\n",
        "std_extra_sparse = sparse.csr_matrix(transformed)\n",
        "\n",
        "tfidf_vecs = sparse.hstack([\n",
        "    # title_vec,\n",
        "    genre_vec,\n",
        "    cast_vec,\n",
        "    director_vec,\n",
        "    plot_vec,\n",
        "]).tocsr()\n",
        "\n",
        "vecs = sparse.hstack([tfidf_vecs, sparse_genome, std_extra_sparse])\n",
        "norm = Normalizer(copy=True)\n",
        "norm_vecs = norm.transform(vecs)\n",
        "norm_vecs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YH44Im9I5dm"
      },
      "source": [
        "We use the `Normalizer` since the cosine similarity between any 2 (tf normalized) vectors is the same as their dot-product. Thus we can now easily get a list of the top n similar movies to a specified movie:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6MvIQO6I5dn"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def top_n_similar_to(movieId: int, n=10) -> OrderedDict:\n",
        "    results = dict()\n",
        "    movie_idx = movies[movies['movieId'] == movieId].index[0]\n",
        "    similarities = norm_vecs.dot(norm_vecs.getrow(movie_idx).transpose()).toarray().flatten()\n",
        "    top_n = similarities.argsort(axis=0)[-n-1:-1][::-1]\n",
        "\n",
        "    for similar_idx in top_n:\n",
        "        title = full_combine.iloc[similar_idx].title\n",
        "        sim = round(similarities[similar_idx] * 100.0, 0)\n",
        "        results[title] = (sim, similar_idx)\n",
        "        \n",
        "    return OrderedDict(sorted(results.items(), key=lambda kv: kv[1], reverse=True))\n",
        "\n",
        "def batch_similar_to(movies: [int], n=10):\n",
        "    results = dict()\n",
        "    for mid in movies:\n",
        "        res = top_n_similar_to(mid, n=n)\n",
        "        for movie, (sim, idx) in res.items():\n",
        "            if (movie in results) and (results[movie][0] < sim):\n",
        "                results[movie] = (sim, idx)\n",
        "            else:\n",
        "                results[movie] = (sim, idx)\n",
        "\n",
        "    return OrderedDict(sorted(results.items(), key=lambda kv: kv[1], reverse=True)[:n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FOeY7pNI5do"
      },
      "source": [
        "And a quick demo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM3QLt7tI5do"
      },
      "source": [
        "avengers = 89745\n",
        "avengers2 = 122892\n",
        "amazing_spiderman = 95510\n",
        "fault_in_stars = 111921\n",
        "\n",
        "import pprint \n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "pp.pprint(top_n_similar_to(avengers))\n",
        "print()\n",
        "pp.pprint(batch_similar_to([amazing_spiderman, fault_in_stars]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh6aaDRmI5dp"
      },
      "source": [
        "Amazing! Well... semi-amazing. It works! We'll keep working to improve the collaborative models since they are in need of the most optimization. Content-based works, but it's not as magical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3hq-3FjI5dq"
      },
      "source": [
        "# Step 4: Who's a good model?\n",
        "\n",
        "Based on the above exploration we'll be sticking to the `SVD` model from now on; We'll provide an additional script with some `SVDpp` stuff in - but it's simply not feasible to train for 35x as long to gain a 1-2% improvement.\n",
        "\n",
        "The following code is copied verbatim from the `scripts/train_model.py` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmWy5CRmI5dr"
      },
      "source": [
        "from surprise.dump import dump\n",
        "\n",
        "random_state = 42\n",
        "\n",
        "reader = Reader(\n",
        "    name=None,\n",
        "    line_format=\"user item rating\",\n",
        "    sep=\",\",\n",
        "    rating_scale=(1, 5),\n",
        "    skip_lines=1,\n",
        ")\n",
        "\n",
        "print(\"Loading data...\")\n",
        "data = Dataset.load_from_file(\"data/train.csv\", reader)\n",
        "\n",
        "print(\"Building training file\")\n",
        "train_set = data.build_full_trainset()\n",
        "\n",
        "del data\n",
        "del reader\n",
        "\n",
        "algo = SVD(random_state=random_state, verbose=True)\n",
        "\n",
        "print(\"Fitting...\")\n",
        "algo.fit(train_set)\n",
        "\n",
        "print(\"Dumping...\")\n",
        "dump(\"model.pkl\", algo=algo, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8hztYo7I5ds"
      },
      "source": [
        "# Step 5: The Proof is in the Pudding\n",
        "\n",
        "Having found the best possible model (within reason), we now have to make a `submission.csv` file to upload to Kaggle for scoring.\n",
        "\n",
        "Once again, the code comes from `scripts/make_submission.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-GVZP9NI5ds"
      },
      "source": [
        "from surprise.dump import load\n",
        "\n",
        "print(\"Loading model...\")\n",
        "_, algo = load(\"model.pkl\")\n",
        "\n",
        "print(\"Loading prediction data...\")\n",
        "test = pd.read_csv(\"data/test.csv\")\n",
        "\n",
        "preds = dict()\n",
        "\n",
        "print(\"Predicting...\")\n",
        "total = len(test)\n",
        "\n",
        "for idx, (uid, mid) in enumerate(test.to_records(index=False)):\n",
        "    _, _, _, est, details = algo.predict(str(uid), str(mid))\n",
        "\n",
        "    if details[\"was_impossible\"]:\n",
        "        print(idx, \"Impossible\")\n",
        "\n",
        "    pred_id = f\"{uid}_{mid}\"\n",
        "    preds[pred_id] = est\n",
        "\n",
        "    if ((idx + 1) % 456_000) == 0:\n",
        "        print(\"[progress]\", round(idx / total * 100.0, 1), \"%\")\n",
        "\n",
        "print(\"Producing output csv...\")\n",
        "pd.Series(preds, name=\"rating\", index=preds.keys()).to_csv(\n",
        "    \"submission.csv\", index_label=\"Id\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnnwpjjoI5du"
      },
      "source": [
        "# Step 6: Goodbye.\n",
        "\n",
        "All that remains is our conclusion\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ZjgqRII5du"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}